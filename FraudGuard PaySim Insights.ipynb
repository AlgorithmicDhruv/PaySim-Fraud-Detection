{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":6594115,"sourceType":"datasetVersion","datasetId":3805493}],"dockerImageVersionId":30732,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-07-13T13:01:00.424126Z","iopub.execute_input":"2024-07-13T13:01:00.424488Z","iopub.status.idle":"2024-07-13T13:01:01.508475Z","shell.execute_reply.started":"2024-07-13T13:01:00.424459Z","shell.execute_reply":"2024-07-13T13:01:01.506977Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/paysim-data/paysim dataset.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd","metadata":{"execution":{"iopub.status.busy":"2024-07-13T13:01:07.664387Z","iopub.execute_input":"2024-07-13T13:01:07.664907Z","iopub.status.idle":"2024-07-13T13:01:07.669631Z","shell.execute_reply.started":"2024-07-13T13:01:07.664874Z","shell.execute_reply":"2024-07-13T13:01:07.668458Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"PATH = \"/kaggle/input/paysim-data/paysim dataset.csv\" \n\n# Read the CSV file into a Pandas DataFrame\ndf = pd.read_csv(PATH)\n\n# Display the some random rows of the DataFrame\ndf.sample(10)","metadata":{"execution":{"iopub.status.busy":"2024-07-13T13:01:10.849142Z","iopub.execute_input":"2024-07-13T13:01:10.849542Z","iopub.status.idle":"2024-07-13T13:01:32.499101Z","shell.execute_reply.started":"2024-07-13T13:01:10.849510Z","shell.execute_reply":"2024-07-13T13:01:32.497733Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"         step      type      amount     nameOrig  oldbalanceOrg  \\\n6124247   540   PAYMENT    23688.13    C18599525       64304.00   \n1505671   143  TRANSFER   254853.52  C2065823705           0.00   \n595749     33   PAYMENT     7063.05   C512684768      150173.39   \n2765211   213   PAYMENT    13571.67   C147413689       88945.00   \n5646730   396  CASH_OUT    71677.03  C1488353176       24668.00   \n3867446   283   CASH_IN   155715.27   C107422891     6680436.70   \n3984847   298  CASH_OUT   143810.17   C862627108           0.00   \n4517883   325  CASH_OUT    65829.37   C116953877       64851.00   \n6187141   569  TRANSFER  1016126.76  C1369867680           0.00   \n2491238   204   CASH_IN   289029.41  C1785585198    11334660.49   \n\n         newbalanceOrig     nameDest  oldbalanceDest  newbalanceDest  isFraud  \\\n6124247        40615.87  M1054008375            0.00            0.00        0   \n1505671            0.00   C409956115      7516377.36      7771230.88        0   \n595749        143110.35   M958056252            0.00            0.00        0   \n2765211        75373.33  M1901171608            0.00            0.00        0   \n5646730            0.00  C1415000082            0.00        71677.03        0   \n3867446      6836151.98  C1801050179       512463.91       356748.64        0   \n3984847            0.00  C1759735495       194725.81       338535.98        0   \n4517883            0.00   C994335556        76214.00       142043.37        0   \n6187141            0.00  C1171803243     11198749.29     12214876.06        0   \n2491238     11623689.91  C1179059245       886578.83       597549.41        0   \n\n         isFlaggedFraud  \n6124247               0  \n1505671               0  \n595749                0  \n2765211               0  \n5646730               0  \n3867446               0  \n3984847               0  \n4517883               0  \n6187141               0  \n2491238               0  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>step</th>\n      <th>type</th>\n      <th>amount</th>\n      <th>nameOrig</th>\n      <th>oldbalanceOrg</th>\n      <th>newbalanceOrig</th>\n      <th>nameDest</th>\n      <th>oldbalanceDest</th>\n      <th>newbalanceDest</th>\n      <th>isFraud</th>\n      <th>isFlaggedFraud</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>6124247</th>\n      <td>540</td>\n      <td>PAYMENT</td>\n      <td>23688.13</td>\n      <td>C18599525</td>\n      <td>64304.00</td>\n      <td>40615.87</td>\n      <td>M1054008375</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1505671</th>\n      <td>143</td>\n      <td>TRANSFER</td>\n      <td>254853.52</td>\n      <td>C2065823705</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>C409956115</td>\n      <td>7516377.36</td>\n      <td>7771230.88</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>595749</th>\n      <td>33</td>\n      <td>PAYMENT</td>\n      <td>7063.05</td>\n      <td>C512684768</td>\n      <td>150173.39</td>\n      <td>143110.35</td>\n      <td>M958056252</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2765211</th>\n      <td>213</td>\n      <td>PAYMENT</td>\n      <td>13571.67</td>\n      <td>C147413689</td>\n      <td>88945.00</td>\n      <td>75373.33</td>\n      <td>M1901171608</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>5646730</th>\n      <td>396</td>\n      <td>CASH_OUT</td>\n      <td>71677.03</td>\n      <td>C1488353176</td>\n      <td>24668.00</td>\n      <td>0.00</td>\n      <td>C1415000082</td>\n      <td>0.00</td>\n      <td>71677.03</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3867446</th>\n      <td>283</td>\n      <td>CASH_IN</td>\n      <td>155715.27</td>\n      <td>C107422891</td>\n      <td>6680436.70</td>\n      <td>6836151.98</td>\n      <td>C1801050179</td>\n      <td>512463.91</td>\n      <td>356748.64</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3984847</th>\n      <td>298</td>\n      <td>CASH_OUT</td>\n      <td>143810.17</td>\n      <td>C862627108</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>C1759735495</td>\n      <td>194725.81</td>\n      <td>338535.98</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4517883</th>\n      <td>325</td>\n      <td>CASH_OUT</td>\n      <td>65829.37</td>\n      <td>C116953877</td>\n      <td>64851.00</td>\n      <td>0.00</td>\n      <td>C994335556</td>\n      <td>76214.00</td>\n      <td>142043.37</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>6187141</th>\n      <td>569</td>\n      <td>TRANSFER</td>\n      <td>1016126.76</td>\n      <td>C1369867680</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>C1171803243</td>\n      <td>11198749.29</td>\n      <td>12214876.06</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2491238</th>\n      <td>204</td>\n      <td>CASH_IN</td>\n      <td>289029.41</td>\n      <td>C1785585198</td>\n      <td>11334660.49</td>\n      <td>11623689.91</td>\n      <td>C1179059245</td>\n      <td>886578.83</td>\n      <td>597549.41</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"df.info()","metadata":{"execution":{"iopub.status.busy":"2024-07-13T07:32:51.040415Z","iopub.execute_input":"2024-07-13T07:32:51.040816Z","iopub.status.idle":"2024-07-13T07:32:51.067891Z","shell.execute_reply.started":"2024-07-13T07:32:51.040785Z","shell.execute_reply":"2024-07-13T07:32:51.066140Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 6362620 entries, 0 to 6362619\nData columns (total 11 columns):\n #   Column          Dtype  \n---  ------          -----  \n 0   step            int64  \n 1   type            object \n 2   amount          float64\n 3   nameOrig        object \n 4   oldbalanceOrg   float64\n 5   newbalanceOrig  float64\n 6   nameDest        object \n 7   oldbalanceDest  float64\n 8   newbalanceDest  float64\n 9   isFraud         int64  \n 10  isFlaggedFraud  int64  \ndtypes: float64(5), int64(3), object(3)\nmemory usage: 534.0+ MB\n","output_type":"stream"}]},{"cell_type":"code","source":"df.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2024-07-13T07:32:51.069709Z","iopub.execute_input":"2024-07-13T07:32:51.070212Z","iopub.status.idle":"2024-07-13T07:32:53.238879Z","shell.execute_reply.started":"2024-07-13T07:32:51.070176Z","shell.execute_reply":"2024-07-13T07:32:53.237678Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"step              0\ntype              0\namount            0\nnameOrig          0\noldbalanceOrg     0\nnewbalanceOrig    0\nnameDest          0\noldbalanceDest    0\nnewbalanceDest    0\nisFraud           0\nisFlaggedFraud    0\ndtype: int64"},"metadata":{}}]},{"cell_type":"markdown","source":"### `type`,`nameOrig` and `nameDest` are of object datatype and every column does not have null values","metadata":{}},{"cell_type":"code","source":"df['isFraud'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2024-07-13T07:32:53.241586Z","iopub.execute_input":"2024-07-13T07:32:53.241963Z","iopub.status.idle":"2024-07-13T07:32:53.317626Z","shell.execute_reply.started":"2024-07-13T07:32:53.241931Z","shell.execute_reply":"2024-07-13T07:32:53.316319Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"isFraud\n0    6354407\n1       8213\nName: count, dtype: int64"},"metadata":{}}]},{"cell_type":"markdown","source":"### This confirms a highly imbalanced dataset typical in fraud detection scenarios.","metadata":{}},{"cell_type":"markdown","source":"> First object type colums","metadata":{}},{"cell_type":"markdown","source":"1. `type`\n2. `nameOrig`\n3. `nameDest`","metadata":{}},{"cell_type":"code","source":"def print_value_count(df,columnName):\n    print(df[columnName].value_counts())\n    print()\n    print(columnName.upper(),\"column has\",len(df[columnName].unique()),\"unique categories..\")","metadata":{"execution":{"iopub.status.busy":"2024-07-13T07:32:53.319440Z","iopub.execute_input":"2024-07-13T07:32:53.319921Z","iopub.status.idle":"2024-07-13T07:32:53.332323Z","shell.execute_reply.started":"2024-07-13T07:32:53.319889Z","shell.execute_reply":"2024-07-13T07:32:53.331047Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"print_value_count(df,'type')","metadata":{"execution":{"iopub.status.busy":"2024-07-13T07:32:53.333865Z","iopub.execute_input":"2024-07-13T07:32:53.334291Z","iopub.status.idle":"2024-07-13T07:32:55.100694Z","shell.execute_reply.started":"2024-07-13T07:32:53.334252Z","shell.execute_reply":"2024-07-13T07:32:55.099228Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"type\nCASH_OUT    2237500\nPAYMENT     2151495\nCASH_IN     1399284\nTRANSFER     532909\nDEBIT         41432\nName: count, dtype: int64\n\nTYPE column has 5 unique categories..\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Given that there are only 5 unique values, one-hot encoding is a feasible approach for `type` column.\n> Now for `nameOrig`\n","metadata":{}},{"cell_type":"code","source":"print_value_count(df,'nameOrig')","metadata":{"execution":{"iopub.status.busy":"2024-07-13T07:32:55.102489Z","iopub.execute_input":"2024-07-13T07:32:55.102917Z","iopub.status.idle":"2024-07-13T07:33:08.966147Z","shell.execute_reply.started":"2024-07-13T07:32:55.102883Z","shell.execute_reply":"2024-07-13T07:33:08.964760Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"nameOrig\nC1902386530    3\nC363736674     3\nC545315117     3\nC724452879     3\nC1784010646    3\n              ..\nC98968405      1\nC720209255     1\nC1567523029    1\nC644777639     1\nC1280323807    1\nName: count, Length: 6353307, dtype: int64\n\nNAMEORIG column has 6353307 unique categories..\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### With 6,353,307 unique values in the `nameOrig` column, it is clear that this column likely represents unique identifiers for each customer or account involved in the transactions. Given the large number of unique values, this column can be considered as having high cardinality.","metadata":{}},{"cell_type":"code","source":"print_value_count(df,'nameDest')","metadata":{"execution":{"iopub.status.busy":"2024-07-13T07:33:08.968496Z","iopub.execute_input":"2024-07-13T07:33:08.968951Z","iopub.status.idle":"2024-07-13T07:33:18.320966Z","shell.execute_reply.started":"2024-07-13T07:33:08.968910Z","shell.execute_reply":"2024-07-13T07:33:18.319433Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"nameDest\nC1286084959    113\nC985934102     109\nC665576141     105\nC2083562754    102\nC248609774     101\n              ... \nM1470027725      1\nM1330329251      1\nM1784358659      1\nM2081431099      1\nC2080388513      1\nName: count, Length: 2722362, dtype: int64\n\nNAMEDEST column has 2722362 unique categories..\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### With 2,722,362 unique values in the `nameDest` column, it is clear that this column likely represents unique identifiers for each customer or account involved in the transactions. Given the large number of unique values, this column can be considered as having high cardinality.","metadata":{}},{"cell_type":"code","source":"print(\"No of rows in data frame:\",df.shape [0])","metadata":{"execution":{"iopub.status.busy":"2024-07-13T07:33:18.322532Z","iopub.execute_input":"2024-07-13T07:33:18.322977Z","iopub.status.idle":"2024-07-13T07:33:18.330082Z","shell.execute_reply.started":"2024-07-13T07:33:18.322936Z","shell.execute_reply":"2024-07-13T07:33:18.328231Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"No of rows in data frame: 6362620\n","output_type":"stream"}]},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2024-07-13T07:33:18.333904Z","iopub.execute_input":"2024-07-13T07:33:18.334368Z","iopub.status.idle":"2024-07-13T07:33:18.359982Z","shell.execute_reply.started":"2024-07-13T07:33:18.334321Z","shell.execute_reply":"2024-07-13T07:33:18.358666Z"},"trusted":true},"execution_count":12,"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"   step      type    amount     nameOrig  oldbalanceOrg  newbalanceOrig  \\\n0     1   PAYMENT   9839.64  C1231006815       170136.0       160296.36   \n1     1   PAYMENT   1864.28  C1666544295        21249.0        19384.72   \n2     1  TRANSFER    181.00  C1305486145          181.0            0.00   \n3     1  CASH_OUT    181.00   C840083671          181.0            0.00   \n4     1   PAYMENT  11668.14  C2048537720        41554.0        29885.86   \n\n      nameDest  oldbalanceDest  newbalanceDest  isFraud  isFlaggedFraud  \n0  M1979787155             0.0             0.0        0               0  \n1  M2044282225             0.0             0.0        0               0  \n2   C553264065             0.0             0.0        1               0  \n3    C38997010         21182.0             0.0        1               0  \n4  M1230701703             0.0             0.0        0               0  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>step</th>\n      <th>type</th>\n      <th>amount</th>\n      <th>nameOrig</th>\n      <th>oldbalanceOrg</th>\n      <th>newbalanceOrig</th>\n      <th>nameDest</th>\n      <th>oldbalanceDest</th>\n      <th>newbalanceDest</th>\n      <th>isFraud</th>\n      <th>isFlaggedFraud</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>PAYMENT</td>\n      <td>9839.64</td>\n      <td>C1231006815</td>\n      <td>170136.0</td>\n      <td>160296.36</td>\n      <td>M1979787155</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>PAYMENT</td>\n      <td>1864.28</td>\n      <td>C1666544295</td>\n      <td>21249.0</td>\n      <td>19384.72</td>\n      <td>M2044282225</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>TRANSFER</td>\n      <td>181.00</td>\n      <td>C1305486145</td>\n      <td>181.0</td>\n      <td>0.00</td>\n      <td>C553264065</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>CASH_OUT</td>\n      <td>181.00</td>\n      <td>C840083671</td>\n      <td>181.0</td>\n      <td>0.00</td>\n      <td>C38997010</td>\n      <td>21182.0</td>\n      <td>0.0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1</td>\n      <td>PAYMENT</td>\n      <td>11668.14</td>\n      <td>C2048537720</td>\n      <td>41554.0</td>\n      <td>29885.86</td>\n      <td>M1230701703</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"### Approach 1: We should drop high cardinality columns:{`nameOrig`,`nameDest`}","metadata":{}},{"cell_type":"markdown","source":"1. Encode the `type` column using one-hot encoding\n","metadata":{}},{"cell_type":"code","source":"# One-hot encode the 'type' column\ndf_encoded = pd.get_dummies(df, columns=['type'])\n\ndf_encoded.head()","metadata":{"execution":{"iopub.status.busy":"2024-07-13T13:01:56.349109Z","iopub.execute_input":"2024-07-13T13:01:56.349809Z","iopub.status.idle":"2024-07-13T13:01:57.887601Z","shell.execute_reply.started":"2024-07-13T13:01:56.349775Z","shell.execute_reply":"2024-07-13T13:01:57.886526Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"   step    amount     nameOrig  oldbalanceOrg  newbalanceOrig     nameDest  \\\n0     1   9839.64  C1231006815       170136.0       160296.36  M1979787155   \n1     1   1864.28  C1666544295        21249.0        19384.72  M2044282225   \n2     1    181.00  C1305486145          181.0            0.00   C553264065   \n3     1    181.00   C840083671          181.0            0.00    C38997010   \n4     1  11668.14  C2048537720        41554.0        29885.86  M1230701703   \n\n   oldbalanceDest  newbalanceDest  isFraud  isFlaggedFraud  type_CASH_IN  \\\n0             0.0             0.0        0               0         False   \n1             0.0             0.0        0               0         False   \n2             0.0             0.0        1               0         False   \n3         21182.0             0.0        1               0         False   \n4             0.0             0.0        0               0         False   \n\n   type_CASH_OUT  type_DEBIT  type_PAYMENT  type_TRANSFER  \n0          False       False          True          False  \n1          False       False          True          False  \n2          False       False         False           True  \n3           True       False         False          False  \n4          False       False          True          False  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>step</th>\n      <th>amount</th>\n      <th>nameOrig</th>\n      <th>oldbalanceOrg</th>\n      <th>newbalanceOrig</th>\n      <th>nameDest</th>\n      <th>oldbalanceDest</th>\n      <th>newbalanceDest</th>\n      <th>isFraud</th>\n      <th>isFlaggedFraud</th>\n      <th>type_CASH_IN</th>\n      <th>type_CASH_OUT</th>\n      <th>type_DEBIT</th>\n      <th>type_PAYMENT</th>\n      <th>type_TRANSFER</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>9839.64</td>\n      <td>C1231006815</td>\n      <td>170136.0</td>\n      <td>160296.36</td>\n      <td>M1979787155</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>1864.28</td>\n      <td>C1666544295</td>\n      <td>21249.0</td>\n      <td>19384.72</td>\n      <td>M2044282225</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>181.00</td>\n      <td>C1305486145</td>\n      <td>181.0</td>\n      <td>0.00</td>\n      <td>C553264065</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>181.00</td>\n      <td>C840083671</td>\n      <td>181.0</td>\n      <td>0.00</td>\n      <td>C38997010</td>\n      <td>21182.0</td>\n      <td>0.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>False</td>\n      <td>True</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1</td>\n      <td>11668.14</td>\n      <td>C2048537720</td>\n      <td>41554.0</td>\n      <td>29885.86</td>\n      <td>M1230701703</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n      <td>False</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"2. Drop `nameOrig` and `nameDest` columns (since they have too many unique categories):","metadata":{}},{"cell_type":"code","source":"df_encoded = df_encoded.drop(columns=['nameOrig', 'nameDest'])","metadata":{"execution":{"iopub.status.busy":"2024-07-13T02:31:43.329669Z","iopub.execute_input":"2024-07-13T02:31:43.329964Z","iopub.status.idle":"2024-07-13T02:31:43.491566Z","shell.execute_reply.started":"2024-07-13T02:31:43.329941Z","shell.execute_reply":"2024-07-13T02:31:43.490478Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"3. Scale the numerical features:","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\n\n# Select numerical columns to scale\nnumerical_cols = ['amount', 'oldbalanceOrg', 'newbalanceOrig', 'oldbalanceDest', 'newbalanceDest']\n\nscaler = StandardScaler()\ndf_encoded[numerical_cols] = scaler.fit_transform(df_encoded[numerical_cols])\n\ndf_encoded.head()","metadata":{"execution":{"iopub.status.busy":"2024-07-13T02:31:43.492789Z","iopub.execute_input":"2024-07-13T02:31:43.493085Z","iopub.status.idle":"2024-07-13T02:31:44.651868Z","shell.execute_reply.started":"2024-07-13T02:31:43.493060Z","shell.execute_reply":"2024-07-13T02:31:44.650707Z"},"trusted":true},"execution_count":16,"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"   step    amount  oldbalanceOrg  newbalanceOrig  oldbalanceDest  \\\n0     1 -0.281560      -0.229810       -0.237622       -0.323814   \n1     1 -0.294767      -0.281359       -0.285812       -0.323814   \n2     1 -0.297555      -0.288654       -0.292442       -0.323814   \n3     1 -0.297555      -0.288654       -0.292442       -0.317582   \n4     1 -0.278532      -0.274329       -0.282221       -0.323814   \n\n   newbalanceDest  isFraud  isFlaggedFraud  type_CASH_IN  type_CASH_OUT  \\\n0       -0.333411        0               0         False          False   \n1       -0.333411        0               0         False          False   \n2       -0.333411        1               0         False          False   \n3       -0.333411        1               0         False           True   \n4       -0.333411        0               0         False          False   \n\n   type_DEBIT  type_PAYMENT  type_TRANSFER  \n0       False          True          False  \n1       False          True          False  \n2       False         False           True  \n3       False         False          False  \n4       False          True          False  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>step</th>\n      <th>amount</th>\n      <th>oldbalanceOrg</th>\n      <th>newbalanceOrig</th>\n      <th>oldbalanceDest</th>\n      <th>newbalanceDest</th>\n      <th>isFraud</th>\n      <th>isFlaggedFraud</th>\n      <th>type_CASH_IN</th>\n      <th>type_CASH_OUT</th>\n      <th>type_DEBIT</th>\n      <th>type_PAYMENT</th>\n      <th>type_TRANSFER</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>-0.281560</td>\n      <td>-0.229810</td>\n      <td>-0.237622</td>\n      <td>-0.323814</td>\n      <td>-0.333411</td>\n      <td>0</td>\n      <td>0</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>-0.294767</td>\n      <td>-0.281359</td>\n      <td>-0.285812</td>\n      <td>-0.323814</td>\n      <td>-0.333411</td>\n      <td>0</td>\n      <td>0</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>-0.297555</td>\n      <td>-0.288654</td>\n      <td>-0.292442</td>\n      <td>-0.323814</td>\n      <td>-0.333411</td>\n      <td>1</td>\n      <td>0</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>-0.297555</td>\n      <td>-0.288654</td>\n      <td>-0.292442</td>\n      <td>-0.317582</td>\n      <td>-0.333411</td>\n      <td>1</td>\n      <td>0</td>\n      <td>False</td>\n      <td>True</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1</td>\n      <td>-0.278532</td>\n      <td>-0.274329</td>\n      <td>-0.282221</td>\n      <td>-0.323814</td>\n      <td>-0.333411</td>\n      <td>0</td>\n      <td>0</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n      <td>False</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"4. Split the dataset into training and testing sets:","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n# Define features and target\nX = df_encoded.drop(columns=['isFraud', 'isFlaggedFraud'])\ny = df_encoded['isFraud']\n\n# Split the dataset\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nprint(X_train.shape, X_test.shape, y_train.shape, y_test.shape)","metadata":{"execution":{"iopub.status.busy":"2024-07-13T02:31:44.653544Z","iopub.execute_input":"2024-07-13T02:31:44.653999Z","iopub.status.idle":"2024-07-13T02:31:46.577738Z","shell.execute_reply.started":"2024-07-13T02:31:44.653958Z","shell.execute_reply":"2024-07-13T02:31:46.576352Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"(5090096, 11) (1272524, 11) (5090096,) (1272524,)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"5. Train a Random Forest Classifier:","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n\n# Initialize the Random Forest model\nrf_model = RandomForestClassifier(random_state=42, n_jobs=-1, class_weight='balanced')\n\n# Train the model\nrf_model.fit(X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2024-07-13T02:31:46.579022Z","iopub.execute_input":"2024-07-13T02:31:46.579392Z","iopub.status.idle":"2024-07-13T02:37:44.526215Z","shell.execute_reply.started":"2024-07-13T02:31:46.579363Z","shell.execute_reply":"2024-07-13T02:37:44.525007Z"},"trusted":true},"execution_count":18,"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"RandomForestClassifier(class_weight='balanced', n_jobs=-1, random_state=42)","text/html":"<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier(class_weight=&#x27;balanced&#x27;, n_jobs=-1, random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(class_weight=&#x27;balanced&#x27;, n_jobs=-1, random_state=42)</pre></div></div></div></div></div>"},"metadata":{}}]},{"cell_type":"markdown","source":"6. Evaluate the Model:","metadata":{}},{"cell_type":"code","source":"# Make predictions\ny_pred = rf_model.predict(X_test)\ny_proba = rf_model.predict_proba(X_test)[:, 1]\n\n# Calculate evaluation metrics\naccuracy = accuracy_score(y_test, y_pred)\nprecision = precision_score(y_test, y_pred)\nrecall = recall_score(y_test, y_pred)\nf1 = f1_score(y_test, y_pred)\nroc_auc = roc_auc_score(y_test, y_proba)\n\nprint(f\"Accuracy: {accuracy:.4f}\")\nprint(f\"Precision: {precision:.4f}\")\nprint(f\"Recall: {recall:.4f}\")\nprint(f\"F1 Score: {f1:.4f}\")\nprint(f\"ROC-AUC: {roc_auc:.4f}\")","metadata":{"execution":{"iopub.status.busy":"2024-07-13T02:37:44.527713Z","iopub.execute_input":"2024-07-13T02:37:44.528337Z","iopub.status.idle":"2024-07-13T02:37:53.481504Z","shell.execute_reply.started":"2024-07-13T02:37:44.528289Z","shell.execute_reply":"2024-07-13T02:37:53.479865Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"Accuracy: 0.9997\nPrecision: 0.9860\nRecall: 0.7840\nF1 Score: 0.8735\nROC-AUC: 0.9943\n","output_type":"stream"}]},{"cell_type":"markdown","source":"#### **Handling Imbalanced Data:** Since recall is slightly lower, we must try SMOTE (Synthetic Minority Over-sampling Technique) to improve recall with class weights (class importance).\n\n> **Class Weights:** Adjusts the importance of each class in the loss function. It doesn't change the data distribution but helps the model focus more on the minority class.\n> **SMOTE:** Generates synthetic samples to balance the dataset, leading to a more balanced class distribution in the training set.","metadata":{}},{"cell_type":"markdown","source":"> Class weights is only used..","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n\n# Define features and target\nX = df_encoded.drop(columns=['isFraud', 'isFlaggedFraud'])\ny = df_encoded['isFraud']\n\n# Split the dataset\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Manually set class weights (e.g., class 1 is 12 times more important)\nclass_weight = {0: 1, 1: 12}\n\n# Initialize the Random Forest model with manual class weights\nrf_model = RandomForestClassifier(random_state=42, n_jobs=-1, class_weight=class_weight)\n\n# Train the model\nrf_model.fit(X_train, y_train)\n\n# Evaluate the model\ny_pred = rf_model.predict(X_test)\ny_proba = rf_model.predict_proba(X_test)[:, 1]\n\naccuracy = accuracy_score(y_test, y_pred)\nprecision = precision_score(y_test, y_pred)\nrecall = recall_score(y_test, y_pred)\nf1 = f1_score(y_test, y_pred)\nroc_auc = roc_auc_score(y_test, y_proba)\n\nprint(f\"Accuracy: {accuracy:.4f}\")\nprint(f\"Precision: {precision:.4f}\")\nprint(f\"Recall: {recall:.4f}\")\nprint(f\"F1 Score: {f1:.4f}\")\nprint(f\"ROC-AUC: {roc_auc:.4f}\")","metadata":{"execution":{"iopub.status.busy":"2024-07-13T04:05:42.078366Z","iopub.execute_input":"2024-07-13T04:05:42.078931Z","iopub.status.idle":"2024-07-13T04:14:00.807879Z","shell.execute_reply.started":"2024-07-13T04:05:42.078881Z","shell.execute_reply":"2024-07-13T04:14:00.806638Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":"Accuracy: 0.9997\nPrecision: 0.9832\nRecall: 0.7969\nF1 Score: 0.8803\nROC-AUC: 0.9943\n","output_type":"stream"}]},{"cell_type":"markdown","source":"#### When comparing the scores from using class weights only versus a vanilla Random Forest model, \n\nIt is evident that using class weights improves recall at the expense of a slight drop in precision. \nThis results in a higher F1 score, indicating a better balance between precision and recall.\n\n","metadata":{}},{"cell_type":"markdown","source":"#### Observations:\n\n1. **Accuracy:** Remains unchanged, which is expected since the dataset is highly imbalanced, and accuracy is dominated by the majority class.\n2. **Precision:** Slightly decreased from 0.9860 to 0.9832.\n3. **Recall:** Improved from 0.7840 to 0.7969, indicating the model is better at identifying fraudulent transactions.\n4. **F1 Score:** Increased from 0.8735 to 0.8803, showing an overall improvement in the model's performance.\n5. **ROC-AUC:** Remains the same at 0.9943, indicating the model's ability to distinguish between classes is unchanged.\n\n-----","metadata":{}},{"cell_type":"markdown","source":"> Combining SMOTE with Class Weights\n","metadata":{}},{"cell_type":"code","source":"from imblearn.over_sampling import SMOTE\n\n# Apply SMote to the training set\nsmote = SMOTE(random_state=42)\nX_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n\n# Initialize the Random Forest model with class weights\nclass_weight = {0: 1, 1: 12}\nrf_model = RandomForestClassifier(random_state=42, n_jobs=-1, class_weight=class_weight)\n\n# Train the model\nrf_model.fit(X_train_resampled, y_train_resampled)\n\n# Evaluate the model\ny_pred = rf_model.predict(X_test)\ny_proba = rf_model.predict_proba(X_test)[:, 1]\n\naccuracy = accuracy_score(y_test, y_pred)\nprecision = precision_score(y_test, y_pred)\nrecall = recall_score(y_test, y_pred)\nf1 = f1_score(y_test, y_pred)\nroc_auc = roc_auc_score(y_test, y_proba)\n\nprint(f\"Accuracy: {accuracy:.4f}\")\nprint(f\"Precision: {precision:.4f}\")\nprint(f\"Recall: {recall:.4f}\")\nprint(f\"F1 Score: {f1:.4f}\")\nprint(f\"ROC-AUC: {roc_auc:.4f}\")","metadata":{"execution":{"iopub.status.busy":"2024-07-13T04:15:44.878939Z","iopub.execute_input":"2024-07-13T04:15:44.882222Z","iopub.status.idle":"2024-07-13T04:38:36.099087Z","shell.execute_reply.started":"2024-07-13T04:15:44.882114Z","shell.execute_reply":"2024-07-13T04:38:36.097341Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stdout","text":"Accuracy: 0.9988\nPrecision: 0.5204\nRecall: 0.9778\nF1 Score: 0.6792\nROC-AUC: 0.9986\n","output_type":"stream"}]},{"cell_type":"markdown","source":"-----","metadata":{}},{"cell_type":"markdown","source":"### Approach 2: Hash high cardinality columns:{`nameOrig`,`nameDest`}","metadata":{}},{"cell_type":"markdown","source":"> Hashing along with class weights without SMOTE","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_extraction import FeatureHasher\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\nimport pandas as pd\n\n# Define features and target\nX = df.drop(columns=['isFraud', 'isFlaggedFraud'])\ny = df['isFraud']\n\n# One-hot encode the 'type' column\nX = pd.get_dummies(X, columns=['type'], drop_first=True)\n\n# Use FeatureHasher for hashing the 'nameOrig' and 'nameDest' columns\nhasher = FeatureHasher(n_features=10, input_type='string')\n\n# Transform the columns\nhashed_nameOrig = hasher.transform(X['nameOrig'].astype(str).apply(lambda x: [x])).toarray()\nhashed_nameDest = hasher.transform(X['nameDest'].astype(str).apply(lambda x: [x])).toarray()\n\n# Convert hashed arrays to DataFrame\nhashed_nameOrig_df = pd.DataFrame(hashed_nameOrig, columns=[f'nameOrig_{i}' for i in range(hashed_nameOrig.shape[1])])\nhashed_nameDest_df = pd.DataFrame(hashed_nameDest, columns=[f'nameDest_{i}' for i in range(hashed_nameDest.shape[1])])\n\n# Drop the original 'nameOrig' and 'nameDest' columns and concatenate the hashed DataFrames\nX = X.drop(columns=['nameOrig', 'nameDest'])\nX = pd.concat([X.reset_index(drop=True), hashed_nameOrig_df, hashed_nameDest_df], axis=1)\n\n# Split the dataset\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Initialize the Random Forest model\nrf_model = RandomForestClassifier(random_state=42, n_jobs=-1, class_weight={0: 1, 1: 12})\n\n# Train the model\nrf_model.fit(X_train, y_train)\n\n# Evaluate the model\ny_pred = rf_model.predict(X_test)\ny_proba = rf_model.predict_proba(X_test)[:, 1]\n\naccuracy = accuracy_score(y_test, y_pred)\nprecision = precision_score(y_test, y_pred)\nrecall = recall_score(y_test, y_pred)\nf1 = f1_score(y_test, y_pred)\nroc_auc = roc_auc_score(y_test, y_proba)\n\nprint(f\"Accuracy: {accuracy:.4f}\")\nprint(f\"Precision: {precision:.4f}\")\nprint(f\"Recall: {recall:.4f}\")\nprint(f\"F1 Score: {f1:.4f}\")\nprint(f\"ROC-AUC: {roc_auc:.4f}\")","metadata":{"execution":{"iopub.status.busy":"2024-07-13T04:58:01.465972Z","iopub.execute_input":"2024-07-13T04:58:01.466654Z","iopub.status.idle":"2024-07-13T05:07:28.151652Z","shell.execute_reply.started":"2024-07-13T04:58:01.466596Z","shell.execute_reply":"2024-07-13T05:07:28.150186Z"},"trusted":true},"execution_count":26,"outputs":[{"name":"stdout","text":"Accuracy: 0.9997\nPrecision: 0.9919\nRecall: 0.7586\nF1 Score: 0.8597\nROC-AUC: 0.9942\n","output_type":"stream"}]},{"cell_type":"markdown","source":"#### Observations:\n\n1. **Accuracy: 0.9997**: This high accuracy suggests that the model is generally performing well, but accuracy alone can be misleading, especially in imbalanced datasets.\n2. **Precision: 0.9919:** This indicates that when the model predicts a transaction as fraudulent, it is correct about 99% of the time. This is very good and suggests that the model is not flagging too many legitimate transactions as fraud.\n3. **Recall: 0.7586:** This value shows that the model is identifying about 76% of actual fraudulent transactions. While decent, there is still room for improvement in capturing more fraud cases.\n4. **F1 Score: 0.8597:** This score balances precision and recall, showing a good trade-off between the two metrics. An F1 score above 0.8 is typically considered strong in many contexts.\n5. **ROC-AUC: 0.9942:** This indicates excellent model performance, as the model can distinguish between classes very effectively across different thresholds.","metadata":{}},{"cell_type":"markdown","source":"> Hashing along with class weights and SMOTE code","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\nfrom sklearn.feature_extraction import FeatureHasher\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.preprocessing import OneHotEncoder\n\n# Define features and target\nX = df.drop(columns=['isFraud', 'isFlaggedFraud'])\ny = df['isFraud']\n\n# One-hot encode the 'type' column\nohe = OneHotEncoder(sparse_output=False)\ntype_encoded = ohe.fit_transform(X[['type']])\n\n# Convert encoded arrays to DataFrame\ntype_encoded_df = pd.DataFrame(type_encoded, columns=ohe.get_feature_names_out(['type']))\n\n# Use FeatureHasher for hashing the 'nameOrig' and 'nameDest' columns\nhasher = FeatureHasher(n_features=10, input_type='string')\nhashed_nameOrig = hasher.transform(X['nameOrig'].apply(lambda x: [x]).tolist()).toarray()\nhashed_nameDest = hasher.transform(X['nameDest'].apply(lambda x: [x]).tolist()).toarray()\n\n# Convert hashed arrays to DataFrame\nhashed_nameOrig_df = pd.DataFrame(hashed_nameOrig, columns=[f'nameOrig_{i}' for i in range(hashed_nameOrig.shape[1])])\nhashed_nameDest_df = pd.DataFrame(hashed_nameDest, columns=[f'nameDest_{i}' for i in range(hashed_nameDest.shape[1])])\n\n# Drop the original 'type', 'nameOrig', and 'nameDest' columns and concatenate the encoded and hashed DataFrames\nX = X.drop(columns=['type', 'nameOrig', 'nameDest'])\nX = pd.concat([X.reset_index(drop=True), type_encoded_df.reset_index(drop=True), hashed_nameOrig_df, hashed_nameDest_df], axis=1)\n\n# Split the dataset\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Apply SMOTE to the training set\nsmote = SMOTE(random_state=42)\nX_train_res, y_train_res = smote.fit_resample(X_train, y_train)\n\n# Initialize the Random Forest model with class weights\nrf_model = RandomForestClassifier(random_state=42, n_jobs=-1, class_weight={0: 1, 1: 12})\n\n# Train the model\nrf_model.fit(X_train_res, y_train_res)\n\n# Evaluate the model\ny_pred = rf_model.predict(X_test)\ny_proba = rf_model.predict_proba(X_test)[:, 1]\n\naccuracy = accuracy_score(y_test, y_pred)\nprecision = precision_score(y_test, y_pred)\nrecall = recall_score(y_test, y_pred)\nf1 = f1_score(y_test, y_pred)\nroc_auc = roc_auc_score(y_test, y_proba)\nconf_matrix = confusion_matrix(y_test, y_pred)\n\nprint(f\"Accuracy: {accuracy:.4f}\")\nprint(f\"Precision: {precision:.4f}\")\nprint(f\"Recall: {recall:.4f}\")\nprint(f\"F1 Score: {f1:.4f}\")\nprint(f\"ROC-AUC: {roc_auc:.4f}\")\nprint(f\"Confusion Matrix:\\n{conf_matrix}\")","metadata":{"execution":{"iopub.status.busy":"2024-07-13T13:02:17.961187Z","iopub.execute_input":"2024-07-13T13:02:17.961540Z","iopub.status.idle":"2024-07-13T13:47:28.695259Z","shell.execute_reply.started":"2024-07-13T13:02:17.961515Z","shell.execute_reply":"2024-07-13T13:47:28.692732Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Accuracy: 0.9996\nPrecision: 0.9895\nRecall: 0.6994\nF1 Score: 0.8195\nROC-AUC: 0.9906\nConfusion Matrix:\n[[1270892      12]\n [    487    1133]]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Trying different models","metadata":{}},{"cell_type":"markdown","source":"> Logistic Regression model (Has faster training time)","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.feature_extraction import FeatureHasher\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n\n# Define features and target\nX = df.drop(columns=['isFraud', 'isFlaggedFraud'])\ny = df['isFraud']\n\n# Use FeatureHasher for hashing the 'nameOrig' and 'nameDest' columns\nhasher = FeatureHasher(n_features=10, input_type='string')\nhashed_nameOrig = hasher.transform(X['nameOrig'].astype(str).apply(lambda x: [x])).toarray()\nhashed_nameDest = hasher.transform(X['nameDest'].astype(str).apply(lambda x: [x])).toarray()\n\n# Convert hashed arrays to DataFrame\nhashed_nameOrig_df = pd.DataFrame(hashed_nameOrig, columns=[f'nameOrig_{i}' for i in range(hashed_nameOrig.shape[1])])\nhashed_nameDest_df = pd.DataFrame(hashed_nameDest, columns=[f'nameDest_{i}' for i in range(hashed_nameDest.shape[1])])\n\n# Drop the original 'nameOrig' and 'nameDest' columns and concatenate the hashed DataFrames\nX = X.drop(columns=['nameOrig', 'nameDest'])\nX = pd.concat([X.reset_index(drop=True), hashed_nameOrig_df, hashed_nameDest_df], axis=1)\n\n# One-hot encode the 'type' column\nX = pd.get_dummies(X, columns=['type'], drop_first=True)\n\n# Split the dataset\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Initialize the Logistic Regression model\nlog_reg = LogisticRegression(max_iter=1000, class_weight={0: 1, 1: 12}, n_jobs=-1, random_state=42)\n\n# Train the model\nlog_reg.fit(X_train, y_train)\n\n# Evaluate the model\ny_pred = log_reg.predict(X_test)\ny_proba = log_reg.predict_proba(X_test)[:, 1]\n\naccuracy = accuracy_score(y_test, y_pred)\nprecision = precision_score(y_test, y_pred)\nrecall = recall_score(y_test, y_pred)\nf1 = f1_score(y_test, y_pred)\nroc_auc = roc_auc_score(y_test, y_proba)\n\nprint(f\"Accuracy: {accuracy:.4f}\")\nprint(f\"Precision: {precision:.4f}\")\nprint(f\"Recall: {recall:.4f}\")\nprint(f\"F1 Score: {f1:.4f}\")\nprint(f\"ROC-AUC: {roc_auc:.4f}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-07-13T14:09:02.405534Z","iopub.execute_input":"2024-07-13T14:09:02.406082Z","iopub.status.idle":"2024-07-13T14:10:36.943640Z","shell.execute_reply.started":"2024-07-13T14:09:02.406047Z","shell.execute_reply":"2024-07-13T14:10:36.942145Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"Accuracy: 0.9937\nPrecision: 0.1157\nRecall: 0.5994\nF1 Score: 0.1939\nROC-AUC: 0.9076\n","output_type":"stream"}]},{"cell_type":"markdown","source":"The results from the Logistic Regression model show that it did not perform as well as the Random Forest models in terms of precision, recall, F1 score, and ROC-AUC. Given the context of your problem and the results from various models, it appears that Random Forest is more suitable for your dataset despite its longer training time.","metadata":{}},{"cell_type":"markdown","source":"> XGBoost (with SMOTE and hashing)","metadata":{}},{"cell_type":"code","source":"import xgboost as xgb\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\nfrom imblearn.over_sampling import SMOTE\n\n# Define features and target\nX = df.drop(columns=['isFraud', 'isFlaggedFraud'])\ny = df['isFraud']\n\n# Use FeatureHasher for hashing the 'nameOrig' and 'nameDest' columns\nhasher = FeatureHasher(n_features=10, input_type='string')\nhashed_nameOrig = hasher.transform(X['nameOrig'].astype(str).apply(lambda x: [x])).toarray()\nhashed_nameDest = hasher.transform(X['nameDest'].astype(str).apply(lambda x: [x])).toarray()\n\n# Convert hashed arrays to DataFrame\nhashed_nameOrig_df = pd.DataFrame(hashed_nameOrig, columns=[f'nameOrig_{i}' for i in range(hashed_nameOrig.shape[1])])\nhashed_nameDest_df = pd.DataFrame(hashed_nameDest, columns=[f'nameDest_{i}' for i in range(hashed_nameDest.shape[1])])\n\n# Drop the original 'nameOrig' and 'nameDest' columns and concatenate the hashed DataFrames\nX = X.drop(columns=['nameOrig', 'nameDest'])\nX = pd.concat([X.reset_index(drop=True), hashed_nameOrig_df, hashed_nameDest_df], axis=1)\n\n# One-hot encode the 'type' column\nX = pd.get_dummies(X, columns=['type'], drop_first=True)\n\n# Split the dataset\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Apply SMOTE to handle class imbalance\nsmote = SMOTE(random_state=42)\nX_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)\n\n# Initialize the XGBoost model\nxgb_model = xgb.XGBClassifier(random_state=42, scale_pos_weight=(len(y_train) - sum(y_train)) / sum(y_train))\n\n# Train the model\nxgb_model.fit(X_train_smote, y_train_smote)\n\n# Evaluate the model\ny_pred = xgb_model.predict(X_test)\ny_proba = xgb_model.predict_proba(X_test)[:, 1]\n\naccuracy = accuracy_score(y_test, y_pred)\nprecision = precision_score(y_test, y_pred)\nrecall = recall_score(y_test, y_pred)\nf1 = f1_score(y_test, y_pred)\nroc_auc = roc_auc_score(y_test, y_proba)\n\nprint(f\"Accuracy: {accuracy:.4f}\")\nprint(f\"Precision: {precision:.4f}\")\nprint(f\"Recall: {recall:.4f}\")\nprint(f\"F1 Score: {f1:.4f}\")\nprint(f\"ROC-AUC: {roc_auc:.4f}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-07-13T14:14:22.921369Z","iopub.execute_input":"2024-07-13T14:14:22.921915Z","iopub.status.idle":"2024-07-13T14:17:43.535185Z","shell.execute_reply.started":"2024-07-13T14:14:22.921876Z","shell.execute_reply":"2024-07-13T14:17:43.533803Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"Accuracy: 0.8021\nPrecision: 0.0063\nRecall: 0.9809\nF1 Score: 0.0125\nROC-AUC: 0.8792\n","output_type":"stream"}]},{"cell_type":"markdown","source":"The XGBoost model, in this case, has a high recall but very low precision, which means it correctly identifies most fraudulent transactions but also has a high number of false positives. This is not ideal for fraud detection, where precision is crucial to minimize the cost of investigating false positives.","metadata":{}},{"cell_type":"markdown","source":"> LightGBM","metadata":{}},{"cell_type":"code","source":"import lightgbm as lgb\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\nfrom imblearn.over_sampling import SMOTE\n\n# Define features and target\nX = df.drop(columns=['isFraud', 'isFlaggedFraud'])\ny = df['isFraud']\n\n# Use FeatureHasher for hashing the 'nameOrig' and 'nameDest' columns\nhasher = FeatureHasher(n_features=10, input_type='string')\nhashed_nameOrig = hasher.transform(X['nameOrig'].astype(str).apply(lambda x: [x])).toarray()\nhashed_nameDest = hasher.transform(X['nameDest'].astype(str).apply(lambda x: [x])).toarray()\n\n# Convert hashed arrays to DataFrame\nhashed_nameOrig_df = pd.DataFrame(hashed_nameOrig, columns=[f'nameOrig_{i}' for i in range(hashed_nameOrig.shape[1])])\nhashed_nameDest_df = pd.DataFrame(hashed_nameDest, columns=[f'nameDest_{i}' for i in range(hashed_nameDest.shape[1])])\n\n# Drop the original 'nameOrig' and 'nameDest' columns and concatenate the hashed DataFrames\nX = X.drop(columns=['nameOrig', 'nameDest'])\nX = pd.concat([X.reset_index(drop=True), hashed_nameOrig_df, hashed_nameDest_df], axis=1)\n\n# One-hot encode the 'type' column\nX = pd.get_dummies(X, columns=['type'], drop_first=True)\n\n# Split the dataset\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Apply SMOTE to handle class imbalance\nsmote = SMOTE(random_state=42)\nX_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)\n\n# Initialize the LightGBM model\nlgb_model = lgb.LGBMClassifier(random_state=42, scale_pos_weight=(len(y_train) - sum(y_train)) / sum(y_train))\n\n# Train the model\nlgb_model.fit(X_train_smote, y_train_smote)\n\n# Evaluate the model\ny_pred = lgb_model.predict(X_test)\ny_proba = lgb_model.predict_proba(X_test)[:, 1]\n\naccuracy = accuracy_score(y_test, y_pred)\nprecision = precision_score(y_test, y_pred)\nrecall = recall_score(y_test, y_pred)\nf1 = f1_score(y_test, y_pred)\nroc_auc = roc_auc_score(y_test, y_proba)\n\nprint(f\"Accuracy: {accuracy:.4f}\")\nprint(f\"Precision: {precision:.4f}\")\nprint(f\"Recall: {recall:.4f}\")\nprint(f\"F1 Score: {f1:.4f}\")\nprint(f\"ROC-AUC: {roc_auc:.4f}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-07-13T14:20:45.004913Z","iopub.execute_input":"2024-07-13T14:20:45.008135Z","iopub.status.idle":"2024-07-13T14:23:28.265175Z","shell.execute_reply.started":"2024-07-13T14:20:45.008070Z","shell.execute_reply":"2024-07-13T14:23:28.263903Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"[LightGBM] [Info] Number of positive: 5083503, number of negative: 5083503\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 2.851040 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 6638\n[LightGBM] [Info] Number of data points in the train set: 10167006, number of used features: 30\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\nAccuracy: 0.9891\nPrecision: 0.1039\nRecall: 0.9938\nF1 Score: 0.1882\nROC-AUC: 0.9960\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Tabular Summary and Conclusion","metadata":{}},{"cell_type":"markdown","source":"| Model                                   | Accuracy | Precision | Recall | F1 Score | ROC-AUC |\n|-----------------------------------------|----------|-----------|--------|----------|---------|\n| Vanilla RF Model                       | 0.9997   | 0.9860    | 0.7840 | 0.8735   | 0.9943  |\n| RF with Class Weights {0: 1, 1: 12}   | 0.9997   | 0.9832    | 0.7969 | 0.8803   | 0.9943  |\n| Combining SMOTE with Class Weights     | 0.9988   | 0.5204    | 0.9778 | 0.6792   | 0.9986  |\n| Hashing with Class Weights             | 0.9997   | 0.9919    | 0.7586 | 0.8597   | 0.9942  |\n| Hashing + Class Weights + SMOTE        | 0.9996   | 0.9895    | 0.6994 | 0.8195   | 0.9906  |\n| Logistic Regression                     | 0.9937   | 0.1157    | 0.5994 | 0.1939   | 0.9076  |\n| XGBoost (with SMOTE and hashing)      | 0.8021   | 0.0063    | 0.9809 | 0.0125   | 0.8792  |\n| LightGBM                               | 0.9891   | 0.1039    | 0.9938 | 0.1882   | 0.9960  |\n","metadata":{}},{"cell_type":"markdown","source":"### **Conclusion:**\n\nFrom the table, the best model to use for PaySim fraud detection appears to be **RF with Class Weights {0: 1, 1: 12}**. This model provides a solid balance of precision and recall while maintaining very high accuracy and ROC-AUC scores. Here’s why:\n\n#### **Performance Metrics:**\n\n- **Accuracy:** 0.9997 (very high)\n- **Precision:** 0.9832 (high)\n- **Recall:** 0.7969 (better than vanilla RF)\n- **F1 Score:** 0.8803 (highest among the models)\n- **ROC-AUC:** 0.9943 (very high)\n\nThis model shows a balanced performance with a notable improvement in recall compared to the vanilla RF model, allowing it to better detect fraudulent transactions without sacrificing much precision. The high F1 score indicates an effective balance between precision and recall, making it a strong candidate for the PaySim fraud detection task.","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}